{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "import numpy as np\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import ClassificationMetric, BinaryLabelDatasetMetric\n",
    "from common_utils import compute_metrics\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "        import load_preproc_data_german\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from aif360.algorithms.inprocessing import MetaFairClassifier\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "dataset_orig = load_preproc_data_german(['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 11)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.])] [array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Training Dataset shape\"))\n",
    "print(dataset_orig.features.shape)\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig.favorable_label, dataset_orig.unfavorable_label)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig.protected_attribute_names)\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig.privileged_protected_attributes, \n",
    "      dataset_orig.unprivileged_protected_attributes)\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into epochs, each with a different group of agents\n",
    "NUM_EPOCHS = 5\n",
    "dataset_orig_epochs = dataset_orig.split(NUM_EPOCHS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 11)\n",
      "(30, 11)\n",
      "Training set: Difference in mean outcomes = -0.063\n",
      "Test set: Difference in mean outcomes = -0.458\n",
      "Best balanced accuracy (no fairness constraints) = 0.5682\n",
      "Optimal classification threshold (no fairness constraints) = 0.8712\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5714\n",
      "Statistical parity difference = -0.1250\n",
      "Disparate impact = 0.0000\n",
      "Average odds difference = -0.0789\n",
      "Equal opportunity difference = -0.1579\n",
      "Theil index = 0.9163\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 11)\n",
      "(30, 11)\n",
      "Training set: Difference in mean outcomes = -0.458\n",
      "Test set: Difference in mean outcomes = -0.057\n",
      "Best balanced accuracy (no fairness constraints) = 0.6136\n",
      "Optimal classification threshold (no fairness constraints) = 0.8613\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5750\n",
      "Statistical parity difference = -0.2727\n",
      "Disparate impact = 0.0000\n",
      "Average odds difference = -0.2381\n",
      "Equal opportunity difference = -0.3333\n",
      "Theil index = 0.7153\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 11)\n",
      "(30, 11)\n",
      "Training set: Difference in mean outcomes = 0.108\n",
      "Test set: Difference in mean outcomes = -0.370\n",
      "Best balanced accuracy (no fairness constraints) = 0.6242\n",
      "Optimal classification threshold (no fairness constraints) = 0.6732\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5250\n",
      "Statistical parity difference = 0.0000\n",
      "Disparate impact = 1.0000\n",
      "Average odds difference = -0.0592\n",
      "Equal opportunity difference = -0.3684\n",
      "Theil index = 0.6134\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 11)\n",
      "(30, 11)\n",
      "Training set: Difference in mean outcomes = 0.018\n",
      "Test set: Difference in mean outcomes = -0.081\n",
      "Best balanced accuracy (no fairness constraints) = 0.7460\n",
      "Optimal classification threshold (no fairness constraints) = 0.6633\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.6411\n",
      "Statistical parity difference = -0.2671\n",
      "Disparate impact = 0.6161\n",
      "Average odds difference = -0.2333\n",
      "Equal opportunity difference = -0.3000\n",
      "Theil index = 0.2310\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Training Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 11)\n",
      "(30, 11)\n",
      "Training set: Difference in mean outcomes = -0.407\n",
      "Test set: Difference in mean outcomes = -0.074\n",
      "Best balanced accuracy (no fairness constraints) = 0.6111\n",
      "Optimal classification threshold (no fairness constraints) = 0.5445\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Test set"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##### Raw predictions - No fairness constraints, only maximizing balanced accuracy"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy = 0.5739\n",
      "Statistical parity difference = -0.4444\n",
      "Disparate impact = 0.4286\n",
      "Average odds difference = -0.5071\n",
      "Equal opportunity difference = -0.3000\n",
      "Theil index = 0.2310\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    dataset_epoch = dataset_orig_epochs[epoch]\n",
    "\n",
    "    # split each data epoch into train and test\n",
    "    dataset_orig_train, dataset_orig_vt = dataset_epoch.split([0.7], shuffle=True)\n",
    "    dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)\n",
    "     \n",
    "    # print out some labels, names, etc.\n",
    "    display(Markdown(\"#### Training Dataset shape\"))\n",
    "    print(dataset_orig_train.features.shape)\n",
    "    print(dataset_orig_valid.features.shape)\n",
    "    \n",
    "    # Logistic regression classifier and predictions\n",
    "    scale_orig = StandardScaler()\n",
    "\n",
    "    # Train\n",
    "    X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "    y_train = dataset_orig_train.labels.ravel()\n",
    "    lmod = LogisticRegression(solver='liblinear')  # Solver specified to avoid future warnings\n",
    "    lmod.fit(X_train, y_train)\n",
    "\n",
    "    # Predict training data\n",
    "    y_train_pred = lmod.predict(X_train)\n",
    "    dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "    dataset_orig_train_pred.labels = y_train_pred\n",
    "\n",
    "    # indices of favorable label\n",
    "    pos_ind = np.where(lmod.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "\n",
    "    # VALIDATION SET\n",
    "    dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "    X_valid = scale_orig.transform(dataset_orig_valid_pred.features)\n",
    "    y_valid = dataset_orig_valid_pred.labels\n",
    "    dataset_orig_valid_pred.scores = lmod.predict_proba(X_valid)[:,pos_ind].reshape(-1,1)\n",
    "\n",
    "    # Predict test data\n",
    "    dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "    X_test = scale_orig.transform(dataset_orig_test_pred.features)\n",
    "    y_test_pred = lmod.predict_proba(X_test)\n",
    "    dataset_orig_test_pred.scores = y_test_pred[:, pos_ind].reshape(-1, 1)\n",
    "\n",
    "    # Evaluate fairness metrics\n",
    "    metric_train = BinaryLabelDatasetMetric(dataset_orig_train_pred, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)\n",
    "    metric_test = BinaryLabelDatasetMetric(dataset_orig_test_pred, \n",
    "                                           unprivileged_groups=unprivileged_groups,\n",
    "                                           privileged_groups=privileged_groups)\n",
    "\n",
    "    print(\"Training set: Difference in mean outcomes = {:.3f}\".format(metric_train.mean_difference()))\n",
    "    print(\"Test set: Difference in mean outcomes = {:.3f}\".format(metric_test.mean_difference()))\n",
    "\n",
    "    # NO FAIRNESS: find best classification threshold\n",
    "\n",
    "    num_thresh = 100\n",
    "    ba_arr = np.zeros(num_thresh)\n",
    "    class_thresh_arr = np.linspace(0.01, 0.99, num_thresh)\n",
    "    for idx, class_thresh in enumerate(class_thresh_arr):\n",
    "        \n",
    "        fav_inds = dataset_orig_valid_pred.scores > class_thresh\n",
    "        dataset_orig_valid_pred.labels[fav_inds] = dataset_orig_valid_pred.favorable_label\n",
    "        dataset_orig_valid_pred.labels[~fav_inds] = dataset_orig_valid_pred.unfavorable_label\n",
    "        \n",
    "        classified_metric_orig_valid = ClassificationMetric(dataset_orig_valid,\n",
    "                                                dataset_orig_valid_pred, \n",
    "                                                unprivileged_groups=unprivileged_groups,\n",
    "                                                privileged_groups=privileged_groups)\n",
    "        \n",
    "        ba_arr[idx] = 0.5*(classified_metric_orig_valid.true_positive_rate()\\\n",
    "                        +classified_metric_orig_valid.true_negative_rate())\n",
    "\n",
    "    best_ind = np.where(ba_arr == np.max(ba_arr))[0][0]\n",
    "    best_class_thresh = class_thresh_arr[best_ind]\n",
    "\n",
    "    print(\"Best balanced accuracy (no fairness constraints) = %.4f\" % np.max(ba_arr))\n",
    "    print(\"Optimal classification threshold (no fairness constraints) = %.4f\" % best_class_thresh)\n",
    "\n",
    "    # Metrics for the test set\n",
    "    fav_inds = dataset_orig_test_pred.scores > best_class_thresh\n",
    "    dataset_orig_test_pred.labels[fav_inds] = dataset_orig_test_pred.favorable_label\n",
    "    dataset_orig_test_pred.labels[~fav_inds] = dataset_orig_test_pred.unfavorable_label\n",
    "\n",
    "    display(Markdown(\"#### Test set\"))\n",
    "    display(Markdown(\"##### Raw predictions - No fairness constraints, only maximizing balanced accuracy\"))\n",
    "\n",
    "    metric_test_bef = compute_metrics(dataset_orig_test, dataset_orig_test_pred, \n",
    "                    unprivileged_groups, privileged_groups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "226",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
